Starting a little late, but that's ok. What are the steps I need to take for an MVP?

    - I should probably skim the paper I'm working off of again, just to refresh my memory.
    - A good starting place would be to replicate the result in that paper, or at least their setup. For that, I need:
        - ALE fully running, with at least one ROM
        - TensorFlow in the most current release, and able to interface with ALE
        - The algorithm from the paper implemented

Let's start with TensorFlow.
Looks like it's working after a simple pip install --upgrade. Somehow that was easier than last time I installed it. I seem to remember having trouble using it in ipython but it works fine now.

Skim the paper?
I think I remember most of the exploration paper now. I also need to go back through the paper I presented, though, because that had most of the details on how to actually do Q-learning.

Maybe a better first step is to replicate the original 2013 DeepMind paper, then add in the model.
Hopefully TensorFlow lets me one-up the original paper by not needing the cropping they used to fit in with the 2D conv library.

Problem: My idea of using the model output as a parallel input into the Q-network doesn't really work, because the Q-network gives outputs for each action and a transition model requires an action as an input. That would mean I would have to instead train Q-networks for each possible action, which is almost definitely not a reasonable thing to attempt given the hardware I'm working with.

Idea: Put the model outputs for each of the actions as an input in the Q-network at the level where the action outputs diverge. Not sure how easy this is in TensorFlow, but probably feasible.

DeepMind algorithm implementation considerations:
    - Need to decide if I'm copying their frame-skipping. Probably good to start with.
    - They convert RGB to grey-scale. TensorFlow probably can't do that. I'll have to find some appropriate Python library, or maaaaaaybe ALE has a built-in capability. A quick search through their github isn't promising.
        - scikit-image looks like a good candidate for a python library

My current overall task is going through ALE and figuring out what it expects as input and output at each step of the process.

Couldn't immediately find a ROM for Pong
Have six of the seven ROMs, now I'm going to figure out how to load them.

Ok, good stuff. I can get a grayscale image from ALE directly. Now trying to figure out downsampling.
I have the grayscaling and downsampling, using scipy. Awesome.

Now what? I think inputting this to TensorFlow.
Actually, going to look at ALE a bit more. TensorFlow seems daunting right now.

Ok maybe I can make some incremental progress with tf. What's the Q-network model architecture? I can go through it piece-by-piece and see how tf would let me create that.

Their input was 84x84x4. 4 is the number of frames they were stacking at a time (because of k=4?). I'm quite confident that I can get tf to do the (better) 110x84x4.

Second layer was 16 8x8 conv filters with stride 4 and a rectifier nonlinearity (is this different from reLU? Doesn't look like). They don't say what exact function but maybe it's in the Nature paper.

Third layer is 32 4x4 conv filters with stride 2 and again a reLU.

Fourth layer is 256 fully connected rectifier units (I'm guessing this is the same as rectifier nonlinearity?).

Output is a fully-connected linear layer with one output per action.

They used RMSProp with size 32 minibatches. Exploration was epsilon-greedy with epsilon annealed linearly from 1 to 0.1 over the first million frames and fixed at 0.1 after. Total of 10 million training frames (per game, I think?) and replay memory of 1 million frames.

I'm currently trying to figure out how to do CNNs in TensorFlow, because that's the first step in the network.

I think I'm going to go with 108x84 because that's divisible by the first-layer stride (4) and then the second-layer stride (2) after subtracting 1

The model architecture is somewhat different in the Nature paper. They have the same input, then 32 8x8 conv stride 4, then 64 4x4 conv stride 2, then 64 3x3 conv stride 1, then 512 fully-connected rectifier, then the action outputs.

It would be nice to figure out how much they trained after each new frame batch.

So it looks like to do the stack thing, I need a 3d conv and kernel. I want to use T_kernel=1 because I really want to do a 2d conv over each frame in the stack.

Think I have 3d conv working. Now I need to build the network.
Network is built, now I need to figure out training.

The Nature paper says they did something interesting for the loss function Q-values. Every C steps (C=10000), they cloned the Q-network and called it Q^{hat}. They use the Q^{hat} values for the loss, so it's:

{(y_{j}-Q(\phi_{j}, a_{j}; \theta))}^{2}, with y_{j}=r_{j} + \gamma \max_{a^{'}}Q^{hat}(\phi_{j+1}, a^{'}; \theta^{-})

This was apparently to increase stability.

So, the algorithm should be something like:

At each step t:
    Select action
    Repeat len(\phi) times:
        Send action to ALE and get reward and frame
    Create 

Ok so I'm getting stuck at the point of constructing phi, the 4-frame (or 3, for Space Invaders) stack. Do they perform updates in the middle? That's what Algorithm 1 in the Nature paper makes it seem like, but I'm highly skeptical. It seems more likely that they just sum the rewards over all the frames in a phi and then update after it's fully created.

Some details on RMSProp: http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf

Trying to figure out how to clone a graph for Q^{hat}

For that matter, trying to figure out how graphs (and multiple graphs) work at all.

Ok, some stuff I've learned:
You build the graph with placeholders, and then pass in images and labels (or rewards, in my case) with feed_dict. The fill_feed_dict function in the MNIST example's fully_connected_feed.py is useful.

I still don't really understand how to clone, but I'll get there eventually. Maybe as_graph_def will be useful.

I'm not sure yet how to stuff a bunch of frame stacks into a batch array. Maybe it can just be done as a numpy operation.

Ok, so one issue I have with the MNIST example is that they're assuming all inputs to the network will be BATCH_SIZE, whereas I need to be able to just input one stack to get Q-values.
Useful: https://www.tensorflow.org/versions/r0.12/resources/faq.html#tensor-shapes

I think I can actually build an MVP without doing the cloning/Q^{hat} thing, so I'm going to do that first. Cloning will be my first step after getting it running.

So, I'm going to get a 32x4 tensor of output Q-values, and a 32x4 tensor of target Q-values. Do I just take the difference and then reduce on that? Well, no, it's the difference squared, but same idea.

It's actually a little more complex. I need the successor state of the Q-values I'm using to calculate a proper Bellman loss. I think that means my history needs to have (s, s', r) entries, where s and s' are frame stacks. To get the loss for a single transition, I do (r + gamma * Q(s') - Q(s))^2, where Q(s') and Q(s) are the max over the Q-values in s' and s, respectively.
That's before I implement cloning.

The above is actually not correct. I need to remember the action taken, too. That's because the update should only be applied to the output corresponding to the action taken, because the transition and reward aren't correct for the other outputs/actions. I need to figure out how to do this in tensorflow. The Q(s') should still be the max over the Q-values in the next state.

This means I really can't do a straightforward batch update. At best, I can group all the sampled transitions that have the same action. I think there's probably some way in tensorflow to just select one of the nodes at a layer and apply an update to that.

So here's how it needs to go:

1) See a frame stack, choose an action, see a next stack and a reward
2) Save all of those things
3) At some later point, sample that transition
4) Calculate the new Q-value of the action you chose in the state
5) Calculate the best Q-value in the next state
6) Do the update to the output node of the action you chose, even if it's no longer the max-Q action

6) seems like the hard part.
There's this function called tf.gather that lets you make a new layer that's pretty much a rearrangement of nodes of the previous layer, so you could rearrange them to only have one node from the previous layer remaining, corresponding to the action that was selected.
That would work, I think, but it would require making O(4) new extensions to the graph at each update step and updating through them. Still, it might be the best way to go.

tf.scatter_update will probably also be required. See http://stackoverflow.com/questions/34935464/update-a-subset-of-weights-in-tensorflow.

Do they train to convergence between frame steps? I doubt it. I'm just going with one update iteration for now.

I'm currently writing the logic for the action-update loop.

I don't think what I did with gather was quite right. It's still giving me a length-4 array.

I think tf.unstack might be closer to what I'm looking for.

I think it's working. I need to work out exactly when I'm using index actions and when I'm using ALE actions.
