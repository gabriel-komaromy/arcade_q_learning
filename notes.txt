Starting a little late, but that's ok. What are the steps I need to take for an MVP?

    - I should probably skim the paper I'm working off of again, just to refresh my memory.
    - A good starting place would be to replicate the result in that paper, or at least their setup. For that, I need:
        - ALE fully running, with at least one ROM
        - TensorFlow in the most current release, and able to interface with ALE
        - The algorithm from the paper implemented

Let's start with TensorFlow.
Looks like it's working after a simple pip install --upgrade. Somehow that was easier than last time I installed it. I seem to remember having trouble using it in ipython but it works fine now.

Skim the paper?
I think I remember most of the exploration paper now. I also need to go back through the paper I presented, though, because that had most of the details on how to actually do Q-learning.

Maybe a better first step is to replicate the original 2013 DeepMind paper, then add in the model.
Hopefully TensorFlow lets me one-up the original paper by not needing the cropping they used to fit in with the 2D conv library.

Problem: My idea of using the model output as a parallel input into the Q-network doesn't really work, because the Q-network gives outputs for each action and a transition model requires an action as an input. That would mean I would have to instead train Q-networks for each possible action, which is almost definitely not a reasonable thing to attempt given the hardware I'm working with.

Idea: Put the model outputs for each of the actions as an input in the Q-network at the level where the action outputs diverge. Not sure how easy this is in TensorFlow, but probably feasible.

DeepMind algorithm implementation considerations:
    - Need to decide if I'm copying their frame-skipping. Probably good to start with.
    - They convert RGB to grey-scale. TensorFlow probably can't do that. I'll have to find some appropriate Python library, or maaaaaaybe ALE has a built-in capability. A quick search through their github isn't promising.
        - scikit-image looks like a good candidate for a python library

My current overall task is going through ALE and figuring out what it expects as input and output at each step of the process.

Couldn't immediately find a ROM for Pong
Have six of the seven ROMs, now I'm going to figure out how to load them.

Ok, good stuff. I can get a grayscale image from ALE directly. Now trying to figure out downsampling.
I have the grayscaling and downsampling, using scipy. Awesome.

Now what? I think inputting this to TensorFlow.
Actually, going to look at ALE a bit more. TensorFlow seems daunting right now.

Ok maybe I can make some incremental progress with tf. What's the Q-network model architecture? I can go through it piece-by-piece and see how tf would let me create that.

Their input was 84x84x4. 4 is the number of frames they were stacking at a time (because of k=4?). I'm quite confident that I can get tf to do the (better) 110x84x4.

Second layer was 16 8x8 conv filters with stride 4 and a rectifier nonlinearity (is this different from reLU? Doesn't look like). They don't say what exact function but maybe it's in the Nature paper.

Third layer is 32 4x4 conv filters with stride 2 and again a reLU.

Fourth layer is 256 fully connected rectifier units (I'm guessing this is the same as rectifier nonlinearity?).

Output is a fully-connected linear layer with one output per action.

They used RMSProp with size 32 minibatches. Exploration was epsilon-greedy with epsilon annealed linearly from 1 to 0.1 over the first million frames and fixed at 0.1 after. Total of 10 million training frames (per game, I think?) and replay memory of 1 million frames.

I'm currently trying to figure out how to do CNNs in TensorFlow, because that's the first step in the network.

I think I'm going to go with 108x84 because that's divisible by the first-layer stride (4) and then the second-layer stride (2) after subtracting 1

The model architecture is somewhat different in the Nature paper. They have the same input, then 32 8x8 conv stride 4, then 64 4x4 conv stride 2, then 64 3x3 conv stride 1, then 512 fully-connected rectifier, then the action outputs.

It would be nice to figure out how much they trained after each new frame batch.

So it looks like to do the stack thing, I need a 3d conv and kernel. I want to use T_kernel=1 because I really want to do a 2d conv over each frame in the stack.

Think I have 3d conv working. Now I need to build the network.
Network is built, now I need to figure out training.

The Nature paper says they did something interesting for the loss function Q-values. Every C steps (C=10000), they cloned the Q-network and called it Q^{hat}. They use the Q^{hat} values for the loss, so it's:

{(y_{j}-Q(\phi_{j}, a_{j}; \theta))}^{2}, with y_{j}=r_{j} + \gamma \max_{a^{'}}Q^{hat}(\phi_{j+1}, a^{'}; \theta^{-})

This was apparently to increase stability.

So, the algorithm should be something like:

At each step t:
    Select action
    Repeat len(\phi) times:
        Send action to ALE and get reward and frame
    Create 

Ok so I'm getting stuck at the point of constructing phi, the 4-frame (or 3, for Space Invaders) stack. Do they perform updates in the middle? That's what Algorithm 1 in the Nature paper makes it seem like, but I'm highly skeptical. It seems more likely that they just sum the rewards over all the frames in a phi and then update after it's fully created.

Some details on RMSProp: http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf

Trying to figure out how to clone a graph for Q^{hat}
