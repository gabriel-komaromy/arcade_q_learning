Starting a little late, but that's ok. What are the steps I need to take for an MVP?

    - I should probably skim the paper I'm working off of again, just to refresh my memory.
    - A good starting place would be to replicate the result in that paper, or at least their setup. For that, I need:
        - ALE fully running, with at least one ROM
        - TensorFlow in the most current release, and able to interface with ALE
        - The algorithm from the paper implemented

Let's start with TensorFlow.
Looks like it's working after a simple pip install --upgrade. Somehow that was easier than last time I installed it. I seem to remember having trouble using it in ipython but it works fine now.

Skim the paper?
I think I remember most of the exploration paper now. I also need to go back through the paper I presented, though, because that had most of the details on how to actually do Q-learning.

Maybe a better first step is to replicate the original 2013 DeepMind paper, then add in the model.
Hopefully TensorFlow lets me one-up the original paper by not needing the cropping they used to fit in with the 2D conv library.

Problem: My idea of using the model output as a parallel input into the Q-network doesn't really work, because the Q-network gives outputs for each action and a transition model requires an action as an input. That would mean I would have to instead train Q-networks for each possible action, which is almost definitely not a reasonable thing to attempt given the hardware I'm working with.

Idea: Put the model outputs for each of the actions as an input in the Q-network at the level where the action outputs diverge. Not sure how easy this is in TensorFlow, but probably feasible.

DeepMind algorithm implementation considerations:
    - Need to decide if I'm copying their frame-skipping. Probably good to start with.
    - They convert RGB to grey-scale. TensorFlow probably can't do that. I'll have to find some appropriate Python library, or maaaaaaybe ALE has a built-in capability. A quick search through their github isn't promising.
        - scikit-image looks like a good candidate for a python library

My current overall task is going through ALE and figuring out what it expects as input and output at each step of the process.

Couldn't immediately find a ROM for Pong
Have six of the seven ROMs, now I'm going to figure out how to load them.

Ok, good stuff. I can get a grayscale image from ALE directly. Now trying to figure out downsampling.
I have the grayscaling and downsampling, using scipy. Awesome.

Now what? I think inputting this to TensorFlow.
Actually, going to look at ALE a bit more. TensorFlow seems daunting right now.

Ok maybe I can make some incremental progress with tf. What's the Q-network model architecture? I can go through it piece-by-piece and see how tf would let me create that.

Their input was 84x84x4. 4 is the number of frames they were stacking at a time (because of k=4?). I'm quite confident that I can get tf to do the (better) 110x84x4.

Second layer was 16 8x8 conv filters with stride 4 and a rectifier nonlinearity (is this different from reLU? Doesn't look like). They don't say what exact function but maybe it's in the Nature paper.

Third layer is 32 4x4 conv filters with stride 2 and again a reLU.

Fourth layer is 256 fully connected rectifier units (I'm guessing this is the same as rectifier nonlinearity?).

Output is a fully-connected linear layer with one output per action.

They used RMSProp with size 32 minibatches. Exploration was epsilon-greedy with epsilon annealed linearly from 1 to 0.1 over the first million frames and fixed at 0.1 after. Total of 10 million training frames (per game, I think?) and replay memory of 1 million frames.
